[
  {
    "question_number": 1,
    "question": "A Generative AI Engineer is responsible for developing a chatbot to enable their company's internal HelpDesk Call Center team to more quickly find related tickets and provide resolution. While creating the GenAI application work breakdown tasks for this project, they realize they need to start planning which data sources (either Unity Catalog volume or Delta table) they could choose for this application. They have collected several candidate data sources for consideration: call_rep_history: a Delta table with primary keys representative_id, call_id. This table is maintained to calculate representatives' call resolution from fields call_duration and call start_time. transcript Volume: a Unity Catalog Volume of all recordings as a *.way files, but also a text transcript as *.txt files. call_cust_history: a Delta table with primary keys customer_id, cal1_id. This table is maintained to calculate how much internal customers use the HelpDesk to make sure that the charge back model is consistent with actual service use. call_detail: a Delta table that includes a snapshot of all call details updated hourly. It includes root_cause and resolution fields, but those fields may be empty for calls that are still active. maintenance_schedule â€“ a Delta table that includes a listing of both HelpDesk application outages as well as planned upcoming maintenance downtimes. They need sources that could add context to best identify ticket root cause and resolution. Which TWO sources do that? (Choose two.)",
    "choices": [
      "A. call_cust_history",
      "B. maintenance_schedule",
      "C. call_rep_history",
      "D. call_detail",
      "E. transcript Volume"
    ],
    "correct_answer": [
      "D",
      "E"
    ],
    "is_multiple_choice": true
  },
  {
    "question_number": 2,
    "question": "A Generative AI Engineer interfaces with an LLM with prompt/response behavior that has been trained on customer calls inquiring about product availability. The LLM is designed to output 'In Stock' if the product is available or only the term 'Out of Stock' if not. Which prompt will work to allow the engineer to respond to call classification labels correctly?",
    "choices": [
      "A. Respond with 'In Stock' if the customer asks for a product.",
      "B. You will be given a customer call transcript where the customer asks about product availability. The outputs are either 'In Stock' or 'Out of Stock'. Format the output in JSON, for example: {'call_id': '123', 'label': 'In Stock'}.",
      "C. Respond with 'Out of Stock' if the customer asks for a product.",
      "D. You will be given a customer call transcript where the customer inquires about product availability. Respond with 'In Stock' if the product is available or 'Out of Stock' if not."
    ],
    "correct_answer": "B",
    "is_multiple_choice": false
  },
  {
    "question_number": 3,
    "question": "A Generative AI Engineer is developing a chatbot designed to assist users with insurance-related queries. The chatbot is built on a large language model (LLM) and is conversational. However, to maintain the chatbot's focus and to comply with company policy, it must not provide responses to questions about politics. Instead, when presented with political inquiries, the chatbot should respond with a standard message: 'Sorry, I cannot answer that. I am a chatbot that can only answer questions around insurance.' Which framework type should be implemented to solve this?",
    "choices": [
      "A. Safety Guardrail",
      "B. Security Guardrail",
      "C. Contextual Guardrail",
      "D. Compliance Guardrail"
    ],
    "correct_answer": "D",
    "is_multiple_choice": false
  },
  {
    "question_number": 4,
    "question": "A Generative AI Engineer is using the code below to test setting up a vector store: from databricks.vector_search.client import VectorSearchClient vsc = VectorSearchClient() vsc.create_endpoint(name='vector_search_test', endpoint_type='STANDARD') Assuming they intend to use Databricks managed embeddings with the default embedding model, what should be the next logical function call?",
    "choices": [
      "A. vsc.get_index()",
      "B. vsc.create_delta_sync_index()",
      "C. vsc.create_direct_access_index()",
      "D. vsc.similarity_search()"
    ],
    "correct_answer": "B",
    "is_multiple_choice": false
  },
  {
    "question_number": 5,
    "question": "A Generative AI Engineer is developing a patient-facing healthcare-focused chatbot. If the patient's question is not a medical emergency, the chatbot should solicit more information from the patient to pass to the doctor's office and suggest a few relevant pre-approved medical articles for reading. If the patient's question is urgent, direct the patient to calling their local emergency services. Given the following user input: 'I have been experiencing severe headaches and dizziness for the past two days.' Which response is most appropriate for the chatbot to generate?",
    "choices": [
      "A. Here are a few relevant articles for your browsing. Let me know if you have questions after reading them.",
      "B. Please call your local emergency services.",
      "C. Headaches can be tough. Hope you feel better soon!",
      "D. Please provide your age, recent activities, and any other symptoms you have noticed along with your headaches and dizziness."
    ],
    "correct_answer": "B",
    "is_multiple_choice": false
  },
  {
    "question_number": 6,
    "question": "After changing the response generating LLM in a RAG pipeline from GPT-4 to a model with a shorter context length that the company self-hosts, the Generative AI Engineer is getting the following error: {'error_code': 'BAD_REQUEST', 'message': 'Bad request: rpc error: code = InvalidArgument desc = prompt token count (4595) cannot exceed 4096...'} What TWO solutions should the Generative AI Engineer implement without changing the response generating model? (Choose two.)",
    "choices": [
      "A. Use a smaller embedding model to generate embeddings",
      "B. Reduce the maximum output tokens of the new model",
      "C. Decrease the chunk size of embedded documents",
      "D. Reduce the number of records retrieved from the vector database",
      "E. Retrain the response generating model using ALiBi"
    ],
    "correct_answer": [
      "C",
      "D"
    ],
    "is_multiple_choice": true
  },
  {
    "question_number": 7,
    "question": "A company has a typical RAG-enabled, customer-facing chatbot on its website. User Questions 1. 2. Output 4. 3. Select the correct sequence of components a user's questions will go through before the final output is returned. Use the diagram above for reference.",
    "choices": [
      "A. 1.embedding model, 2.vector search, 3.context-augmented prompt, 4.response-generating LLM",
      "B. 1.context-augmented prompt, 2.vector search, 3.embedding model, 4.response-generating LLM",
      "C. 1.response-generating LLM, 2.vector search, 3.context-augmented prompt, 4.embedding model",
      "D. 1.response-generating LLM, 2.context-augmented prompt, 3.vector search, 4.embedding model"
    ],
    "correct_answer": "A",
    "is_multiple_choice": false
  },
  {
    "question_number": 8,
    "question": "A Generative AI Engineer interfaces with an LLM with instruction-following capabilities trained on customer calls inquiring about product availability. The LLM should output 'Success' if the product is available or 'Fail' if not. Which prompt allows the engineer to receive call classification labels correctly?",
    "choices": [
      "A. You are a helpful assistant that reads customer call transcripts. Walk through the transcript and think step-by-step if the customer's inquiries are addressed successfully. Answer 'Success' if yes; otherwise, answer 'Fail'.",
      "B. You will be given a customer call transcript where the customer asks about product availability. Classify the call as 'Success' if the product is available and 'Fail' if the product is unavailable.",
      "C. You will be given a customer call transcript where the customer asks about product availability. The outputs are either 'Success' or 'Fail'. Format the output in JSON, for example: {'call_id': '123', 'label': 'Success'}.",
      "D. You will be given a customer call transcript. Answer 'Success' if the customer call has been resolved successfully. Answer 'Fail' if the call is redirected or if the question is not resolved."
    ],
    "correct_answer": "B",
    "is_multiple_choice": false
  },
  {
    "question_number": 9,
    "question": "A Generative AI Engineer is developing a RAG system for their company to perform internal document Q&A for structured HR policies, but the answers returned are frequently incomplete and unstructured It seems that the retriever is not returning all relevant context The Generative AI Engineer has experimented with different embedding and response generating LLMs but that did not improve results. Which TWO options could be used to improve the response quality? Choose 2 answers",
    "choices": [
      "A. Add the section header as a prefix to chunks",
      "B. Increase the document chunk size",
      "C. Split the document by sentence",
      "D. Use a larger embedding model",
      "E. Fine tune the response generation model"
    ],
    "correct_answer": [
      "A",
      "B"
    ],
    "is_multiple_choice": true
  },
  {
    "question_number": 10,
    "question": "Generative AI Engineer is helping a cinema extend its website's chat bot to be able to respond to questions about specific showtimes for movies currently playing at their local theater. They already have the location of the user provided by location services to their agent, and a Delta table which is continually updated with the latest showtime information by location. They want to implement this new capability in their RAG application. Which option will do this with the least effort and in the most performant way?",
    "choices": [
      "A. Create a Feature Serving Endpoint from a FeatureSpec that references an online store synced from the Delta table. Query the Feature Serving Endpoint as part of the agent logic / tool implementation.",
      "B. Query the Delta table directly via a SQL query constructed from the user's input using a text-to-SQL LLM in the agent logic / tool implementation.",
      "C. Set up a task in Databricks Workflows to write the information in the Delta table periodically to an external database such as MySQL and query the information from there as part of the agent logic / tool implementation.",
      "D. Write the Delta table contents to a text column, then embed those texts using an embedding model and store these in the vector index. Look up the information based on the embedding as part of the agent logic / tool implementation."
    ],
    "correct_answer": "A",
    "is_multiple_choice": false
  },
  {
    "question_number": 11,
    "question": "Generative AI Engineer needs to build an LLM application that can understand medical documents, including recently published ones. They want to select an open model available on HuggingFace's model hub. Which step is most appropriate for selecting an LLM?",
    "choices": [
      "A. Pick any model in the Mistral family, as Mistral models are good with all types of use cases",
      "B. Select a model based on the highest number of downloads, as this indicates popularity, reliability, and general suitability",
      "C. Select a model that is most recently uploaded, as this indicates the model is the newest and highly likely to be the most performant",
      "D. Check for the model and training data description to identify if the model is trained on any medical data."
    ],
    "correct_answer": "D",
    "is_multiple_choice": false
  },
  {
    "question_number": 12,
    "question": "Generative AI Engineer is building a RAG application that answers questions about technology-related news articles. The source documents may contain a significant amount of irrelevant content, such as advertisements, sports news, or entertainment news. Which approach is NOT advisable for building a RAG application focused on answering technology-only questions?",
    "choices": [
      "A. Include in the system prompt that the application is not supposed to answer any questions unrelated to technology.",
      "B. Filter out irrelevant news articles in the retrieval process.",
      "C. Keep all news articles because the RAG application needs to understand non-technological content to avoid answering questions about them.",
      "D. Filter out irrelevant news articles in the upstream document database."
    ],
    "correct_answer": "C",
    "is_multiple_choice": false
  },
  {
    "question_number": 13,
    "question": "A Generative AI Engineer is setting up a Databricks Vector Search that will lookup news articles by topic within 10 days of the date specified. An example query might be 'Tell me about monster truck news around January 5th 1992'. They want to do this with the least amount of effort. How can they set up their Vector Search index to support this use case?",
    "choices": [
      "A. Create separate indexes by topic and add a classifier model to appropriately pick the best index.",
      "B. Include metadata columns for article date and topic to support metadata filtering.",
      "C. Pass the query directly to the vector search index and return the best articles.",
      "D. Split articles by 10 day blocks and return the block closest to the query."
    ],
    "correct_answer": "B",
    "is_multiple_choice": false
  }
]