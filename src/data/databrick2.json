[
  {
    "question_number": 1,
    "question": "A Generative AI Engineer has created a RAG application to look up answers to questions about a series of fantasy novels that are being asked on the author’s web forum. The fantasy novel texts are chunked and embedded into a vector store with metadata (page number, chapter number, book title), retrieved with the user’s query, and provided to an LLM for response generation. The Generative AI Engineer used their intuition to pick the chunking strategy and associated configurations but now wants to more methodically choose the best values. Which TWO strategies should the Generative AI Engineer take to optimize their chunking strategy and parameters? (Choose two.)",
    "choices": [
      "A. Change embedding models and compare performance.",
      "B. Add a classifier for user queries that predicts which book will best contain the answer. Use this to filter retrieval.",
      "C. Choose an appropriate evaluation metric (such as recall or NDCG) and experiment with changes in the chunking strategy, such as splitting chunks by paragraphs or chapters. Choose the strategy that gives the best performance metric.",
      "D. Pass known questions and best answers to an LLM and instruct the LLM to provide the best token count. Use a summary statistic (mean, median, etc.) of the best token counts to choose chunk size.",
      "E. Create an LLM-as-a-judge metric to evaluate how well previous questions are answered by the most appropriate chunk. Optimize the chunking parameters based upon the values of the metric."
    ],
    "correct_answer": "CE",
    "is_multiple_choice": true
  },
  {
    "question_number": 2,
    "question": "A Generative AI Engineer has a provisioned throughput model serving endpoint as part of a RAG application and would like to monitor the serving endpoint’s incoming requests and outgoing responses. The current approach is to include a micro-service in between the endpoint and the user interface to write logs to a remote server. Which Databricks feature should they use instead which will perform the same task?",
    "choices": [
      "A. Vector Search",
      "B. Lakeview",
      "C. DBSQL",
      "D. Inference Tables"
    ],
    "correct_answer": "D",
    "is_multiple_choice": false
  },
  {
    "question_number": 3,
    "question": "A Generative AI Engineer is responsible for developing a chatbot to enable their company’s internal HelpDesk Call Center team to more quickly find related tickets and provide resolution. While creating the GenAI application work breakdown tasks for this project, they realize they need to start planning which data sources (either Unity Catalog volume or Delta table) they could choose for this application. They have collected several candidate data sources for consideration. Which TWO sources do that? (Choose two.)",
    "choices": [
      "A. call_cust_history",
      "B. maintenance_schedule",
      "C. call_rep_history",
      "D. call_detail",
      "E. transcript Volume"
    ],
    "correct_answer": "DE",
    "is_multiple_choice": true
  },
  {
    "question_number": 4,
    "question": "When developing an LLM application, it’s crucial to ensure that the data used for training the model complies with licensing requirements to avoid legal risks. Which action is NOT appropriate to avoid legal risks?",
    "choices": [
      "A. Reach out to the data curators directly before you have started using the trained model to let them know.",
      "B. Use any available data you personally created which is completely original and you can decide what license to use.",
      "C. Only use data explicitly labeled with an open license and ensure the license terms are followed.",
      "D. Reach out to the data curators directly after you have started using the trained model to let them know."
    ],
    "correct_answer": "D",
    "is_multiple_choice": false
  },
  {
    "question_number": 5,
    "question": "A Generative AI Engineer has developed an LLM application to answer questions about internal company policies. The Generative AI Engineer must ensure that the application doesn’t hallucinate or leak confidential data. Which approach should NOT be used to mitigate hallucination or confidential data leakage?",
    "choices": [
      "A. Add guardrails to filter outputs from the LLM before it is shown to the user",
      "B. Fine-tune the model on your data, hoping it will learn what is appropriate and not",
      "C. Limit the data available based on the user’s access level",
      "D. Use a strong system prompt to ensure the model aligns with your needs."
    ],
    "correct_answer": "B",
    "is_multiple_choice": false
  },
  {
    "question_number": 6,
    "question": "A Generative AI Engineer interfaces with an LLM with prompt/response behavior that has been trained on customer calls inquiring about product availability. The LLM is designed to output 'In Stock' if the product is available or only the term 'Out of Stock' if not. Which prompt will work to allow the engineer to respond to call classification labels correctly?",
    "choices": [
      "A. Respond with 'In Stock' if the customer asks for a product.",
      "B. You will be given a customer call transcript where the customer asks about product availability. The outputs are either 'In Stock' or 'Out of Stock'. Format the output in JSON, for example: 'call_id': '123', 'label': 'In Stock'.",
      "C. Respond with 'Out of Stock' if the customer asks for a product.",
      "D. You will be given a customer call transcript where the customer inquires about product availability. Respond with 'In Stock' if the product is available or 'Out of Stock' if not."
    ],
    "correct_answer": "D",
    "is_multiple_choice": false
  },
  {
    "question_number": 7,
    "question": "A Generative AI Engineer is tasked with developing a RAG application that will help a small internal group of experts at their company answer specific questions, augmented by an internal knowledge base. They want the best possible quality in the answers, and neither latency nor throughput is a huge concern given that the user group is small and they’re willing to wait for the best answer. The topics are sensitive in nature and the data is highly confidential and so, due to regulatory requirements, none of the information is allowed to be transmitted to third parties. Which model meets all the Generative AI Engineer’s needs in this situation?",
    "choices": [
      "A. Dolly 1.5B",
      "B. OpenAI GPT-4",
      "C. BGE-large",
      "D. Llama2-70B"
    ],
    "correct_answer": "D",
    "is_multiple_choice": false
  },
  {
    "question_number": 8,
    "question": "A Generative AI Engineer would like an LLM to generate formatted JSON from emails. This will require parsing and extracting the following information: order ID, date, and sender email. They will need to write a prompt that will extract the relevant information in JSON format with the highest level of output accuracy. Which prompt will do that?",
    "choices": [
      "A. You will receive customer emails and need to extract date, sender email, and order ID. You should return the date, sender email, and order ID information in JSON format.",
      "B. You will receive customer emails and need to extract date, sender email, and order ID. Return the extracted information in JSON format. Here’s an example: 'date': 'April 16, 2024', 'sender_email': '[email protected]', 'order_id': 'RE987D'",
      "C. You will receive customer emails and need to extract date, sender email, and order ID. Return the extracted information in a human-readable format.",
      "D. You will receive customer emails and need to extract date, sender email, and order IReturn the extracted information in JSON format."
    ],
    "correct_answer": "A",
    "is_multiple_choice": false
  },
  {
    "question_number": 9,
    "question": "A Generative AI Engineer is creating an agent-based LLM system for their favorite monster truck team. The system can answer text based questions about the monster truck team, lookup event dates via an API call, or query tables on the team’s latest standings. How could the Generative AI Engineer best design these capabilities into their system?",
    "choices": [
      "A. Ingest PDF documents about the monster truck team into a vector store and query it in a RAG architecture.",
      "B. Write a system prompt for the agent listing available tools and bundle it into an agent system that runs a number of calls to solve a query.",
      "C. Instruct the LLM to respond with 'RAG', 'API', or 'TABLE' depending on the query, then use text parsing and conditional statements to resolve the query.",
      "D. Build a system prompt with all possible event dates and table information in the system prompt. Use a RAG architecture to lookup generic text questions and otherwise leverage the information in the system prompt."
    ],
    "correct_answer": "A",
    "is_multiple_choice": false
  },
  {
    "question_number": 10,
    "question": "A Generative AI Engineer has been asked to design an LLM-based application that accomplishes the following business objective: answer employee HR questions using HR PDF documentation. Which set of high level tasks should the Generative AI Engineer’s system perform?",
    "choices": [
      "A. Calculate averaged embeddings for each HR document, compare embeddings to user query to find the best document. Pass the best document with the user query into an LLM with a large context window to generate a response to the employee.",
      "B. Use an LLM to summarize HR documentation. Provide summaries of documentation and user query into an LLM with a large context window to generate a response to the user.",
      "C. Create an interaction matrix of historical employee questions and HR documentation. Use ALS to factorize the matrix and create embeddings. Calculate the embeddings of new queries and use them to find the best HR documentation. Use an LLM to generate a response to the employee question based upon the documentation retrieved.",
      "D. Split HR documentation into chunks and embed into a vector store. Use the employee question to retrieve best matched chunks of documentation, and use the LLM to generate a response to the employee based upon the documentation retrieved."
    ],
    "correct_answer": "D",
    "is_multiple_choice": false
  },
  {
    "question_number": 11,
    "question": "Generative AI Engineer at an electronics company just deployed a RAG application for customers to ask questions about products that the company carries. However, they received feedback that the RAG response often returns information about an irrelevant product. What can the engineer do to improve the relevance of the RAG’s response?",
    "choices": [
      "A. Assess the quality of the retrieved context",
      "B. Implement caching for frequently asked questions",
      "C. Use a different LLM to improve the generated response",
      "D. Use a different semantic similarity search algorithm"
    ],
    "correct_answer": "D",
    "is_multiple_choice": false
  },
  {
    "question_number": 12,
    "question": "A Generative AI Engineer is developing a chatbot designed to assist users with insurance-related queries. The chatbot is built on a large language model (LLM) and is conversational. However, to maintain the chatbot’s focus and to comply with company policy, it must not provide responses to questions about politics. Instead, when presented with political inquiries, the chatbot should respond with a standard message: 'Sorry, I cannot answer that. I am a chatbot that can only answer questions around insurance.' Which framework type should be implemented to solve this?",
    "choices": [
      "A. Safety Guardrail",
      "B. Security Guardrail",
      "C. Contextual Guardrail",
      "D. Compliance Guardrail"
    ],
    "correct_answer": "A",
    "is_multiple_choice": false
  },
  {
    "question_number": 13,
    "question": "A Generative AI Engineer is using the code below to test setting up a vector store: from databricks.vector_search.client import VectorSearchClient; vsc = VectorSearchClient(); vsc.create_endpoint(name='vector_search_test', endpoint_type='STANDARD'); Assuming they intend to use Databricks managed embeddings with the default embedding model, what should be the next logical function call?",
    "choices": [
      "A. vsc.get_index()",
      "B. vsc.create_delta_sync_index()",
      "C. vsc.create_direct_access_index()",
      "D. vsc.similarity_search()"
    ],
    "correct_answer": "B",
    "is_multiple_choice": false
  },
  {
    "question_number": 14,
    "question": "A Generative AI Engineer is tasked with deploying an application that takes advantage of a custom MLflow Pyfunc model to return some interim results. How should they configure the endpoint to pass the secrets and credentials?",
    "choices": [
      "A. Use spark.conf.set()",
      "B. Pass variables using the Databricks Feature Store API",
      "C. Add credentials using environment variables",
      "D. Pass the secrets in plain text"
    ],
    "correct_answer": "C",
    "is_multiple_choice": false
  },
  {
    "question_number": 15,
    "question": "A Generative AI Engineer is developing a patient-facing healthcare-focused chatbot. If the patient’s question is not a medical emergency, the chatbot should solicit more information from the patient to pass to the doctor’s office and suggest a few relevant pre-approved medical articles for reading. If the patient’s question is urgent, direct the patient to calling their local emergency services. Given the following user input: 'I have been experiencing severe headaches and dizziness for the past two days.' Which response is most appropriate for the chatbot to generate?",
    "choices": [
      "A. Here are a few relevant articles for your browsing. Let me know if you have questions after reading them.",
      "B. Please call your local emergency services.",
      "C. Headaches can be tough. Hope you feel better soon!",
      "D. Please provide your age, recent activities, and any other symptoms you have noticed along with your headaches and dizziness."
    ],
    "correct_answer": "D",
    "is_multiple_choice": false
  },
  {
    "question_number": 16,
    "question": "After changing the response generating LLM in a RAG pipeline from GPT-4 to a model with a shorter context length that the company self-hosts, the Generative AI Engineer is getting the following error: {'error_code': 'BAD_REQUEST', 'message': 'Bad request: rpc error: code = InvalidArgument desc = prompt token count (4595) cannot exceed 4096...'} What TWO solutions should the Generative AI Engineer implement without changing the response generating model? (Choose two.)",
    "choices": [
      "A. Use a smaller embedding model to generate embeddings",
      "B. Reduce the maximum output tokens of the new model",
      "C. Decrease the chunk size of embedded documents",
      "D. Reduce the number of records retrieved from the vector database",
      "E. Retrain the response generating model using ALiBi"
    ],
    "correct_answer": "CD",
    "is_multiple_choice": true
  },
  {
    "question_number": 17,
    "question": "A Generative AI Engineer has a provisioned throughput model serving endpoint as part of a RAG application and would like to monitor the serving endpoint’s incoming requests and outgoing responses. Which Databricks feature should they use?",
    "choices": [
      "A. AutoML",
      "B. Vector Search",
      "C. Inference Tables",
      "D. Feature Serving"
    ],
    "correct_answer": "B",
    "is_multiple_choice": false
  },
  {
    "question_number": 18,
    "question": "When developing an LLM application, it’s crucial to ensure that the data used for training the model complies with licensing requirements to avoid legal risks. Which action is most appropriate to avoid legal risks?",
    "choices": [
      "A. Only use data explicitly labeled with an open license and ensure the license terms are followed.",
      "B. Any LLM outputs are reasonable to use because they do not reveal the original sources of data directly.",
      "C. Reach out to the data curators directly to gain written consent for using their data.",
      "D. Use any publicly available data as public data does not have legal restrictions."
    ],
    "correct_answer": "B",
    "is_multiple_choice": false
  },
  {
    "question_number": 19,
    "question": "A Generative AI Engineer interfaces with an LLM with instruction-following capabilities trained on customer calls inquiring about product availability. The LLM should output 'Success' if the product is available or 'Fail' if not. Which prompt allows the engineer to receive call classification labels correctly?",
    "choices": [
      "A. You are a helpful assistant that reads customer call transcripts. Walk through the transcript and think step-by-step if the customer’s inquiries are addressed successfully. Answer 'Success' if yes; otherwise, answer 'Fail'.",
      "B. You will be given a customer call transcript where the customer asks about product availability. Classify the call as 'Success' if the product is available and 'Fail' if the product is unavailable.",
      "C. You will be given a customer call transcript where the customer asks about product availability. The outputs are either 'Success' or 'Fail'. Format the output in JSON, for example: 'call_id': '123', 'label': 'Success'.",
      "D. You will be given a customer call transcript. Answer 'Success' if the customer call has been resolved successfully. Answer 'Fail' if the call is redirected or if the question is not resolved."
    ],
    "correct_answer": "B",
    "is_multiple_choice": false
  },
  {
    "question_number": 20,
    "question": "A Generative AI Engineer would like an LLM to parse and extract the following information: date, sender email, and order ID. The output should be formatted into JSON. They need a prompt that will extract and output the required information in JSON with the highest level of output accuracy. Which prompt will do that?",
    "choices": [
      "A. You will receive customer emails and need to extract date, sender email, and order ID. Return the extracted information in a human-readable format.",
      "B. You will receive customer emails and need to extract date, sender email, and order ID. Return the extracted information in JSON format.",
      "C. You will receive customer emails and need to extract date, sender email, and order ID. Return the extracted information in JSON format. Here’s an example: 'date': 'April 16, 2024', 'sender_email': '[email protected]', 'order_id': 'RE987D'",
      "D. You will receive customer emails and need to extract date, sender email, and order IYou should return the date, sender email, and order ID information in JSON format."
    ],
    "correct_answer": "C",
    "is_multiple_choice": false
  },
  {
    "question_number": 21,
    "question": "A Generative AI Engineer has built an LLM-based system that will automatically translate user text between two languages. They now want to benchmark multiple LLM’s on this task and pick the best one. They have an evaluation set with known high quality translation examples. They want to evaluate each LLM using the evaluation set with a performant metric. Which metric should they choose for this evaluation?",
    "choices": [
      "A. BLEU metric",
      "B. NDCG metric",
      "C. ROUGE metric",
      "D. RECALL metric"
    ],
    "correct_answer": "A",
    "is_multiple_choice": false
  },
  {
    "question_number": 22,
    "question": "Generative AI Engineer is helping a cinema extend its website's chat bot to be able to respond to questions about specific showtimes for movies currently playing at their local theater. They already have the location of the user provided by location services to their agent, and a Delta table which is continually updated with the latest showtime information by location. They want to implement this new capability in their RAG application. Which option will do this with the least effort and in the most performant way?",
    "choices": [
      "A. Create a Feature Serving Endpoint from a FeatureSpec that references an online store synced from the Delta table. Query the Feature Serving Endpoint as part of the agent logic / tool implementation.",
      "B. Query the Delta table directly via a SQL query constructed from the user's input using a text-to-SQL LLM in the agent logic / tool implementation.",
      "C. Set up a task in Databricks Workflows to write the information in the Delta table periodically to an external database such as MySQL and query the information from there as part of the agent logic / tool implementation.",
      "D. Write the Delta table contents to a text column, then embed those texts using an embedding model and store these in the vector index. Look up the information based on the embedding as part of the agent logic / tool implementation."
    ],
    "correct_answer": "A",
    "is_multiple_choice": false
  },
  {
    "question_number": 23,
    "question": "Generative AI Engineer needs to build an LLM application that can understand medical documents, including recently published ones. They want to select an open model available on HuggingFace’s model hub. Which step is most appropriate for selecting an LLM?",
    "choices": [
      "A. Pick any model in the Mistral family, as Mistral models are good with all types of use cases",
      "B. Select a model based on the highest number of downloads, as this indicates popularity, reliability, and general suitability",
      "C. Select a model that is most recently uploaded, as this indicates the model is the newest and highly likely to be the most performant",
      "D. Check for the model and training data description to identify if the model is trained on any medical data."
    ],
    "correct_answer": "B",
    "is_multiple_choice": false
  },
  {
    "question_number": 24,
    "question": "Generative AI Engineer is building a RAG application that answers questions about technology-related news articles. The source documents may contain a significant amount of irrelevant content, such as advertisements, sports news, or entertainment news. Which approach is NOT advisable for building a RAG application focused on answering technology-only questions?",
    "choices": [
      "A. Include in the system prompt that the application is not supposed to answer any questions unrelated to technology.",
      "B. Filter out irrelevant news articles in the retrieval process.",
      "C. Keep all news articles because the RAG application needs to understand non-technological content to avoid answering questions about them.",
      "D. Filter out irrelevant news articles in the upstream document database."
    ],
    "correct_answer": "C",
    "is_multiple_choice": false
  },
  {
    "question_number": 25,
    "question": "A Generative AI Engineer is setting up a Databricks Vector Search that will lookup news articles by topic within 10 days of the date specified. An example query might be 'Tell me about monster truck news around January 5th 1992'. They want to do this with the least amount of effort. How can they set up their Vector Search index to support this use case?",
    "choices": [
      "A. Create separate indexes by topic and add a classifier model to appropriately pick the best index.",
      "B. Include metadata columns for article date and topic to support metadata filtering.",
      "C. Pass the query directly to the vector search index and return the best articles.",
      "D. Split articles by 10 day blocks and return the block closest to the query."
    ],
    "correct_answer": "B",
    "is_multiple_choice": false
  },
  {
    "question_number": 26,
    "question": "A Generative AI Engineer at a home appliance company has been asked to design an LLM based application that accomplishes the following business objective: answer customer questions on home appliances using the associated instruction manuals. Which set of high-level tasks should the Generative AI Engineer’s system perform?",
    "choices": [
      "A. Split instruction manuals into chunks and embed into a vector store. Use the question to retrieve best matched chunks of manual, and use the LLM to generate a response to the user based upon the manual retrieved.",
      "B. Create an interaction matrix of historical user questions and appliance instruction manuals. Use ALS to factorize the matrix and create embeddings. Calculate the embeddings of new queries and use them to find the best manual. Use an LLM to generate a response to the question based upon the manual retrieved.",
      "C. Calculate averaged embeddings for each instruction manual, compare embeddings to user query to find the best manual. Pass the best manual with user query into an LLM with a large context window to generate a response to the employee.",
      "D. Use an LLM to summarize all of the instruction manuals. Provide summaries of each manual and user query into an LLM with a large context window to generate a response to the user."
    ],
    "correct_answer": "A",
    "is_multiple_choice": false
  },
  {
    "question_number": 27,
    "question": "A Generative AI Engineer has created a RAG application which can help employees interpret HR documentation. The prototype application is now working with some positive feedback from internal company testers. Now the Generative AI Engineer wants to formally evaluate the system’s performance and understand where to focus their efforts to further improve the system. How should the Generative AI Engineer evaluate the system?",
    "choices": [
      "A. Use ROUGE score to comprehensively evaluate the quality of the final generated answers.",
      "B. Use an LLM-as-a-judge to evaluate the quality of the final answers generated.",
      "C. Curate a dataset that can test the retrieval and generation components of the system separately. Use MLflow’s built in evaluation metrics to perform the evaluation on the retrieval and generation components.",
      "D. Benchmark multiple LLMs with the same data and pick the best LLM for the job."
    ],
    "correct_answer": "C",
    "is_multiple_choice": false
  }
]