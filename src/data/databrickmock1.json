[
  {
    "question_number": 1,
    "question": "You are developing a real-time customer support chatbot that needs to handle thousands of concurrent user queries with low latency. The chatbot should provide concise and accurate responses while maintaining cost efficiency. Which LLM would be the best choice for this application?",
    "choices": [
      "A small, optimized model with fewer parameters, fine-tuned for real-time conversational AI",
      "A transformer model trained on a massive dataset without any fine-tuning",
      "A large model with the highest accuracy on open-ended creative tasks, regardless of latency",
      "A large-scale model with 175 billion parameters fine-tuned for general-purpose Q&A"
    ],
    "correct_answer": "A",
    "is_multiple_choice": false
  },
  {
    "question_number": 2,
    "question": "A Databricks engineer is developing a generative AI assistant for recommendations and task execution, such as retrieving data insights. To ensure correct function calls, the engineer must design a structured prompt template that exposes available functions. Which prompt design best ensures accurate function selection based on user input?",
    "choices": [
      "Define each function explicitly in the prompt, including descriptions, input parameters, and example use cases.",
      "Use generic placeholders in the prompt (e.g., <function_name>) and let the model predict the function names at runtime.",
      "Allow the assistant to infer function names dynamically without specifying available functions in the prompt.",
      "Define the function calls in a separate configuration file and reference them in the prompt without listing their descriptions."
    ],
    "correct_answer": "A",
    "is_multiple_choice": false
  },
  {
    "question_number": 3,
    "question": "You are developing a continuous integration and deployment (CI/CD) pipeline for a generative AI application in Databricks. Which TWO practices should be followed to ensure a reliable and automated deployment process?",
    "choices": [
      "Use Databricks Repos to manage source code and integrate with Git-based CI/CD workflows.",
      "Manually package and deploy models by downloading them and uploading them to the production cluster.",
      "Automate testing and validation of model predictions using Databricks Jobs before deploying to production.",
      "Deploy models directly from Jupyter notebooks without tracking changes, as Databricks handles versioning automatically.",
      "Use MLflow Model Registry to track different model versions and automate model transitions between stages."
    ],
    "correct_answer": "A, E",
    "is_multiple_choice": true
  },
  {
    "question_number": 4,
    "question": "You are developing a Generative AI-powered chatbot that assists users in troubleshooting cloud infrastructure issues. To improve response accuracy, you need to augment the user's prompt by extracting key fields, terms, and intents from their query. Which approach is the most effective way to dynamically augment a prompt with relevant context?",
    "choices": [
      "Require users to manually input detailed context instead of automatically extracting relevant details.",
      "Use a fixed set of predefined intents without analyzing the user's specific input.",
      "Append a generic troubleshooting guide to every prompt before passing it to the model.",
      "Extract entities from the user query and dynamically add relevant details such as the cloud service name and error code to the prompt."
    ],
    "correct_answer": "D",
    "is_multiple_choice": false
  },
  {
    "question_number": 5,
    "question": "You have implemented a retrieval mechanism for your Databricks-based generative AI system and need to ensure it retrieves all relevant documents from a knowledge base. Which metric is most suitable for evaluating whether the system retrieves all relevant information?",
    "choices": [
      "TF-IDF Score",
      "Perplexity",
      "Recall@K",
      "Levenshtein Distance"
    ],
    "correct_answer": "C",
    "is_multiple_choice": false
  },
  {
    "question_number": 6,
    "question": "You are choosing between two LLM architectures: a transformer-based model with 6 billion parameters and a distilled version of the same model with 1.3 billion parameters. Your primary goal is to maximize cost efficiency while maintaining reasonable performance for a document summarization task. Which TWO metrics should be the main deciding factors?",
    "choices": [
      "Number of Training Epochs",
      "Compute Cost per Token",
      "Hardware Utilization",
      "BLEU Score",
      "Parameter Count"
    ],
    "correct_answer": "B, D",
    "is_multiple_choice": true
  },
  {
    "question_number": 7,
    "question": "A team is fine-tuning a generative AI model for sentiment analysis. They need to prepare high-quality prompt-response pairs that align with the model's task. Given the objective of classifying text into positive, negative, or neutral sentiments, which of the following prompt-response pairs is the most appropriate for model training?",
    "choices": [
      "Prompt: \"Write a product review\" → Response: \"This laptop has a fast processor, good storage, and a bright screen\"",
      "Prompt: \"What is your favorite product?\" → Response: \"My favorite product is the XYZ Smartwatch because it tracks my steps accurately\"",
      "Prompt: \"Describe the features of the new ABC phone\" → Response: \"It has a 120Hz display, a 5000mAh battery, and 8GB RAM\"",
      "Prompt: \"How do you feel about this product?\" → Response: \"I love it! The battery lasts long, and the sound quality is great\""
    ],
    "correct_answer": "D",
    "is_multiple_choice": false
  },
  {
    "question_number": 8,
    "question": "You are building a machine learning pipeline in Databricks using MLflow. You want to wrap a trained model inside a PyFunc model so that you can apply custom pre-processing and post-processing logic. Which of the following steps is essential when implementing a PyFunc model in MLflow?",
    "choices": [
      "Implement a class inheriting from mlflow.pyfunc.PythonModel and override the predict method.",
      "Store the PyFunc model in MLflow as an experiment rather than as an artifact.",
      "The PyFunc model requires a conda environment to run and cannot be deployed using a virtual environment.",
      "The predict method inside the PyFunc model must always return a Pandas DataFrame."
    ],
    "correct_answer": "A",
    "is_multiple_choice": false
  },
  {
    "question_number": 9,
    "question": "A team of developers is using a large language model (LLM) to generate customer support responses. They notice that the LLM's default responses are overly verbose and lack conciseness. They want to adjust the model's output to make responses more concise while maintaining accuracy and helpfulness. Which of the following prompts is the most effective way to achieve this adjustment?",
    "choices": [
      "Generate a response that balances thoroughness and brevity, allowing the customer to get the necessary information quickly",
      "Summarize your response in no more than two sentences while keeping the key details accurate and clear",
      "Respond in a conversational tone with as much detail as possible, ensuring the response is engaging and informative",
      "Provide a detailed response to the customer's inquiry, ensuring all aspects of the question are covered"
    ],
    "correct_answer": "B",
    "is_multiple_choice": false
  },
  {
    "question_number": 10,
    "question": "You are developing a chatbot that assists data scientists in selecting the most suitable AI model based on a given use case. The chatbot takes a natural language description of a task (e.g., \"I need an AI model for image classification\") and returns the best model type (e.g., CNN, Vision Transformer). Which chain components would be most appropriate for this chatbot's functionality?",
    "choices": [
      "RAG Retriever → LLM → Output Parser",
      "Embedding Model → Vector Search → LLM → JSON Formatter",
      "Text Preprocessor → LLM → Vector Database → JSON Formatter",
      "Text Preprocessor → LLM → Output Parser"
    ],
    "correct_answer": "A",
    "is_multiple_choice": false
  },
  {
    "question_number": 11,
    "question": "A healthcare provider is deploying a Generative AI assistant that answers patient inquiries about medical conditions and treatments. During testing, several potential safety concerns arise. Which approach would best assess response safety and mitigate risks?",
    "choices": [
      "Prioritizing AI-generated responses that contain disclaimers but still provide speculative health diagnoses.",
      "Allowing the AI to generate treatment recommendations based on common internet searches.",
      "Configuring the AI to refuse to answer any health-related question, even when providing general awareness information.",
      "Ensuring that the AI avoids providing direct medical advice and instead refers users to qualified healthcare professionals."
    ],
    "correct_answer": "D",
    "is_multiple_choice": false
  },
  {
    "question_number": 12,
    "question": "A Generative AI Engineer is designing an LLM-powered e-commerce chatbot that uses an external retrieval system. The engineer needs an embedding model that balances cost and latency over quality for high-volume, low-stakes queries. Which choice best meets these requirements?",
    "choices": [
      "Embedding Model Z: Context length 2048, smallest model size 6GB, embedding dimension 1536",
      "Embedding Model X: Context length 256, smallest model size 0.08GB, embedding dimension 256",
      "Embedding Model P: Context length 8192, smallest model size 12GB, embedding dimension 3072",
      "Embedding Model Y: Context length 1024, smallest model size 2GB, embedding dimension 1024"
    ],
    "correct_answer": "B",
    "is_multiple_choice": false
  },
  {
    "question_number": 13,
    "question": "You are working on a Generative AI application that needs to efficiently retrieve semantically similar documents from a large dataset. To enable vector search in Databricks, what is the correct sequence of steps for creating a Vector Search index?",
    "choices": [
      "Convert text data into embeddings → Store embeddings in a Delta Table → Create a Vector Search index → Query the index using a similarity search",
      "Generate embeddings using a separate API → Store embeddings in a JSON file → Query the JSON file using a full-text search",
      "Store raw text data directly in a Delta Table → Create a Vector Search index → Query the index using a similarity search",
      "Convert text data into embeddings → Store embeddings in a relational database → Create a Vector Search index → Query the index using SQL joins"
    ],
    "correct_answer": "A",
    "is_multiple_choice": false
  },
  {
    "question_number": 14,
    "question": "A company is deploying a machine learning model using Databricks Model Serving, and they need to restrict access to the model endpoint so that only authorized applications can send requests. Which of the following strategies should they implement to secure access to the endpoint?",
    "choices": [
      "Configure fine-grained access controls using Databricks Access Control Lists (ACLs) for Model Serving endpoints",
      "Use Databricks Personal Access Tokens (PATs) for authentication and restrict usage to specific IP ranges",
      "Expose the model serving endpoint as a public API and rely on application-level security checks",
      "Disable token-based authentication and use network security groups (NSGs) alone to restrict access"
    ],
    "correct_answer": "A",
    "is_multiple_choice": false
  },
  {
    "question_number": 15,
    "question": "A developer is building a semantic search system in Databricks using a Vector Search index. They need to find the most similar embeddings to a given input query efficiently. Which SQL function should they use when querying a Vector Search index in Databricks?",
    "choices": [
      "img:uploads/assessment-quiz/files/choices/question_bank/6912ef546e7a7.png",
      "img:uploads/assessment-quiz/files/choices/question_bank/6912ef5c63e0f.png",
      "img:uploads/assessment-quiz/files/choices/question_bank/6912f0351e42a.png",
      "img:uploads/assessment-quiz/files/choices/question_bank/6912f05d352de.png"
    ],
    "correct_answer": "D",
    "is_multiple_choice": false
  },
  {
    "question_number": 16,
    "question": "You are developing a customer support chatbot in Databricks that leverages a large language model (LLM). During testing, you notice that responses vary significantly based on how the question is phrased. Which of the following is the best approach to ensure the model provides more consistent and relevant responses?",
    "choices": [
      "Use structured prompts with system messages to guide response formatting",
      "Avoid including examples in the prompt to prevent bias in responses",
      "Reduce the length of the prompt to make it more concise",
      "Increase the model temperature to improve response stability"
    ],
    "correct_answer": "A",
    "is_multiple_choice": false
  },
  {
    "question_number": 17,
    "question": "A Generative AI Engineer is optimizing a retrieval-augmented generation (RAG) pipeline by storing document embeddings in a vector database. They notice that the database is running out of capacity due to an excessive number of stored vectors. Which TWO actions can they take to reduce storage utilization without significantly impacting retrieval accuracy?",
    "choices": [
      "Reduce the document chunk overlap",
      "Increase the dimensionality of embeddings",
      "Convert embeddings to lower precision (e.g., float32 to float16)",
      "Reduce the number of tokens per chunk",
      "Increase the number of vectors stored per document"
    ],
    "correct_answer": "A, D",
    "is_multiple_choice": true
  },
  {
    "question_number": 18,
    "question": "A data engineer is selecting a pre-trained generative AI model from a model hub for a customer support chatbot. The engineer is reviewing the model card to ensure the model is appropriate for the task. Which TWO of the following model metadata fields are most relevant when making the selection?",
    "choices": [
      "The GPU architecture used for training the model",
      "Model latency and inference speed benchmarks",
      "Intended use cases and known limitations",
      "Training dataset details and potential biases"
    ],
    "correct_answer": "C, D",
    "is_multiple_choice": true
  },
  {
    "question_number": 19,
    "question": "A company is deploying a Retrieval-Augmented Generation (RAG) application on Databricks to serve real-time recommendations. The application needs to efficiently manage feature serving, embedding retrieval, and LLM inference while ensuring low latency. Which of the following choices best supports optimized feature serving in a production-grade RAG application?",
    "choices": [
      "Deploy embeddings in a vector database, use a caching layer for frequently retrieved data, and serve the LLM model via a managed endpoint.",
      "Store embeddings in a feature store, retrieve documents from a key-value store, and serve the model via an on-premise API.",
      "Train a transformer model from scratch, store embeddings in cloud object storage, and use SQL queries for retrieval.",
      "Store all documents as JSON in a NoSQL database, perform real-time feature extraction on each query, and serve using a Kubernetes-based LLM."
    ],
    "correct_answer": "A",
    "is_multiple_choice": false
  },
  {
    "question_number": 20,
    "question": "A team of engineers is testing various prompt formats for a text generation model in Databricks. They observe that slight modifications in phrasing result in significant variations in the output. Which of the following statements best explains how different prompt formats influence model behavior?",
    "choices": [
      "Using ambiguous phrasing in a prompt forces the model to provide a more diverse set of answers.",
      "Changing the prompt from a question to a statement does not affect the response quality or content.",
      "Structuring a prompt with clear instructions and examples can lead to more accurate and relevant outputs.",
      "Prompt length has no effect on model responses as the model only focuses on keywords."
    ],
    "correct_answer": "C",
    "is_multiple_choice": false
  },
  {
    "question_number": 21,
    "question": "A company is developing a large language model using datasets that may contain sensitive information, such as personally identifiable information (PII) and copyrighted content. Which of the following practices best ensures compliance with legal and licensing requirements?",
    "choices": [
      "Implement a data filtering pipeline that detects and removes sensitive and copyrighted information before training",
      "Encrypt all training data to mitigate privacy concerns, even if the data contains PII",
      "Assume that all data sourced from third-party vendors is legally compliant and can be used without review",
      "Anonymize sensitive data only after training is complete, since model training does not retain raw data"
    ],
    "correct_answer": "A",
    "is_multiple_choice": false
  },
  {
    "question_number": 22,
    "question": "A Generative AI Engineer is developing a RAG application that extracts and processes structured text from a collection of HTML documents. The documents contain embedded <p>, <h1>, and <h2> tags with relevant textual content, and the engineer wants a solution that allows for easy parsing and manipulation of the document structure. Which Python package should be used to extract and parse the text efficiently?",
    "choices": [
      "pdfplumber",
      "beautifulsoup",
      "re (regular expressions)",
      "pytesseract"
    ],
    "correct_answer": "B",
    "is_multiple_choice": false
  },
  {
    "question_number": 23,
    "question": "A law firm is building a Generative AI-powered legal assistant that provides guidance on contracts and regulations. To prevent the model from generating incorrect or misleading legal advice, what is the best guardrail to implement?",
    "choices": [
      "Retrieval-Augmented Generation (RAG) with verified legal documents",
      "Using a higher temperature setting to allow for more diverse answers",
      "Disabling stop sequences to allow for longer, more detailed responses",
      "Fine-tuning the model exclusively on past user queries"
    ],
    "correct_answer": "A",
    "is_multiple_choice": false
  },
  {
    "question_number": 24,
    "question": "You are selecting a model for automatically summarizing long research articles. You examine two models in the Databricks Model Hub and compare their model cards: Model A (Abstractive summarization, trained on academic papers and news, generates concise summaries, may introduce minor errors) vs Model B (Extractive summarization, trained on business reports and legal documents, preserves exact wording, summaries may be lengthy). Which model should you choose for generating concise, high-level summaries of research articles?",
    "choices": [
      "Model B, because extractive summarization ensures accuracy",
      "Model A, because academic papers require high-level summarization rather than exact sentences",
      "Model A, because it is designed for abstractive summarization",
      "Neither, because both models have limitations that make them unsuitable for research article summarization"
    ],
    "correct_answer": "C",
    "is_multiple_choice": false
  },
  {
    "question_number": 25,
    "question": "A Generative AI Engineer is designing a retrieval-augmented generation (RAG) system where the retriever must process document chunks efficiently while keeping inference cost and latency low. The documents have been preprocessed into chunks of 256 tokens each, and the engineer must select the best model configuration based on cost and speed rather than maximum accuracy. Which of the following context lengths and model configurations best aligns with the engineer's requirements?",
    "choices": [
      "Context length 256: smallest model is 0.08GB with an embedding dimension of 256",
      "Context length 512: smallest model is 0.40GB with an embedding dimension of 512",
      "Context length 2048: smallest model is 6GB with an embedding dimension of 2048",
      "Context length 8192: smallest model is 10GB with an embedding dimension of 3072"
    ],
    "correct_answer": "A",
    "is_multiple_choice": false
  },
  {
    "question_number": 26,
    "question": "You are building a Generative AI pipeline in Databricks and need to code a chain using a PyFunc model with pre-processing and post-processing steps. Which TWO approaches will correctly implement this workflow?",
    "choices": [
      "Skip pre-processing entirely and rely on the model to handle variations in input format.",
      "Implement a pre-processing step that extracts relevant features from raw input before passing them to the model.",
      "Use a PyFunc model to directly wrap a pre-trained model without modifying the input or output.",
      "Store the pre-processing logic separately in a Jupyter notebook and manually run it before inference.",
      "Define a custom Python function that normalizes input data before passing it to the model and formats the output for downstream consumption."
    ],
    "correct_answer": "B, E",
    "is_multiple_choice": true
  },
  {
    "question_number": 27,
    "question": "You are building a simple prompt chaining pipeline in Databricks that takes a user query, reformats it, passes it to an LLM for response generation, and then summarizes the response. Which of the following best represents the correct sequence of operations in a simple chain?",
    "choices": [
      "User query → Reformat query → Response generation → Summarize",
      "User query → Response generation → Summarize → Reformat query",
      "User query → Response generation → Reformat query → Summarize",
      "User query → Summarize → Response generation → Reformat query"
    ],
    "correct_answer": "A",
    "is_multiple_choice": false
  },
  {
    "question_number": 28,
    "question": "You are deploying a fine-tuned generative AI model on Databricks and need to ensure efficient inference serving while optimizing resource utilization. Which deployment strategy is most appropriate for real-time inference with low-latency requirements?",
    "choices": [
      "Serve the model as a REST API using Databricks Model Serving",
      "Deploy the model using Databricks Jobs with a batch inference pipeline",
      "Store the model in Unity Catalog and retrieve it dynamically in a notebook session for each request",
      "Deploy the model as a Spark UDF for parallelized processing of inference requests"
    ],
    "correct_answer": "A",
    "is_multiple_choice": false
  },
  {
    "question_number": 29,
    "question": "You have deployed a Generative AI model in Databricks, and you need to implement an effective monitoring strategy to ensure model performance, fairness, and reliability over time. Which TWO monitoring approaches are most effective for tracking model quality and detecting performance degradation?",
    "choices": [
      "Logging inference latency and tracking response times",
      "Disabling logging to reduce storage costs and inference overhead",
      "Monitoring drift in input data distribution over time",
      "Running the model periodically on a fixed benchmark dataset but ignoring production data",
      "Relying solely on real-time user feedback for model evaluation"
    ],
    "correct_answer": "A, C",
    "is_multiple_choice": true
  },
  {
    "question_number": 30,
    "question": "A data engineer is working on a Databricks application that needs to extract large volumes of structured and semi-structured data from a cloud-based data lake. Which of the following tools is the most appropriate for efficient data extraction while ensuring scalability and fault tolerance?",
    "choices": [
      "Apache Spark DataFrame API",
      "Pandas DataFrame",
      "SQLAlchemy",
      "Python CSV Reader (csv module)"
    ],
    "correct_answer": "A",
    "is_multiple_choice": false
  },
  {
    "question_number": 31,
    "question": "Your team is deploying a generative AI model on Databricks for a cloud-based application. The model will handle unpredictable request loads, sometimes experiencing peak demand while remaining idle at other times. What is the best approach to balance cost efficiency and performance?",
    "choices": [
      "Deploy the model on a Databricks job cluster with an autoscaling policy that adjusts based on workload",
      "Package the model into a custom Docker container and deploy it manually on a VM-based Kubernetes cluster",
      "Use a Databricks GPU cluster that runs continuously to ensure minimal cold start times",
      "Deploy the model on a persistent high-availability cluster to eliminate latency issues"
    ],
    "correct_answer": "A",
    "is_multiple_choice": false
  },
  {
    "question_number": 32,
    "question": "A company is deploying a generative AI model to summarize long legal documents into concise summaries while preserving key information. The team has evaluated three models using ROUGE and BERTScore metrics. Model X (ROUGE-1: 42.3, ROUGE-2: 30.7, ROUGE-L: 39.8, BERTScore: 0.82), Model Y (ROUGE-1: 44.1, ROUGE-2: 32.5, ROUGE-L: 41.3, BERTScore: 0.86), Model Z (ROUGE-1: 40.8, ROUGE-2: 29.9, ROUGE-L: 38.2, BERTScore: 0.79). Which model should be selected for deployment?",
    "choices": [
      "Model Y",
      "Any model, since the differences in scores are minor",
      "Model Z",
      "Model X"
    ],
    "correct_answer": "A",
    "is_multiple_choice": false
  },
  {
    "question_number": 33,
    "question": "You are developing an AI-powered summarization tool for legal and financial documents that require precise, factually accurate summaries with minimal hallucination. Which type of LLM would be the most appropriate for this task?",
    "choices": [
      "A fine-tuned LLM specializing in legal and financial document summarization",
      "A smaller LLM optimized for chat-based interactions rather than summarization",
      "A low-parameter LLM designed for real-time conversational agents",
      "A large, general-purpose LLM without fine-tuning but capable of generating coherent text"
    ],
    "correct_answer": "A",
    "is_multiple_choice": false
  },
  {
    "question_number": 34,
    "question": "You are designing an AI-powered development assistant that helps software engineers by generating code snippets, explaining code, and refactoring functions. The model must provide high-quality code suggestions while maintaining efficiency in a production environment. Which LLM is the best choice for this application?",
    "choices": [
      "Codex",
      "Mistral-7B",
      "BERT",
      "GPT-3.5-Turbo"
    ],
    "correct_answer": "A",
    "is_multiple_choice": false
  },
  {
    "question_number": 35,
    "question": "A Retrieval-Augmented Generation (RAG) application is being built to generate legal document summaries. However, the data source contains biased or offensive language, which could negatively affect the application's output. Which of the following approaches is the best way to mitigate problematic text while ensuring high-quality retrieval?",
    "choices": [
      "Use a content moderation API to flag and filter problematic text before storing embeddings.",
      "Train a custom model to detect problematic phrases and remove the entire document from retrieval.",
      "Rely on the language model's built-in toxicity filters to avoid generating problematic text.",
      "Preprocess the text by removing all words that could be offensive using a predefined blacklist."
    ],
    "correct_answer": "A",
    "is_multiple_choice": false
  },
  {
    "question_number": 36,
    "question": "Why is inference logging important when assessing the performance of a deployed Retrieval-Augmented Generation (RAG) application in Databricks?",
    "choices": [
      "It helps track user interactions and improve the quality of document retrieval",
      "It prevents bias in model responses by filtering out specific user inputs automatically",
      "It automatically fine-tunes the model in real time based on user feedback",
      "It optimizes the storage of retrieved documents to minimize infrastructure costs"
    ],
    "correct_answer": "A",
    "is_multiple_choice": false
  },
  {
    "question_number": 37,
    "question": "You need to store and serve vector embeddings for a RAG application in Databricks. Which storage solution is best suited for efficient retrieval and serving of these embeddings?",
    "choices": [
      "Databricks Vector Search",
      "Databricks Jobs",
      "Amazon S3",
      "Delta Lake"
    ],
    "correct_answer": "A",
    "is_multiple_choice": false
  },
  {
    "question_number": 38,
    "question": "You have deployed a pyfunc model that generates text responses using a Transformer-based LLM. The model outputs raw text, but you need to perform additional filtering to remove offensive content before presenting the final output to users. Which of the following approaches correctly implements post-processing in a pyfunc model?",
    "choices": [
      "Modify the predict() method to include a text filtering step before returning responses.",
      "Apply a pre-processing step that removes offensive content before sending data to the pyfunc model.",
      "Store the model output in Delta Lake and run a separate Databricks Job to filter offensive content.",
      "Deploy a separate API-based filtering service and integrate it downstream of the pyfunc model's output."
    ],
    "correct_answer": "A",
    "is_multiple_choice": false
  },
  {
    "question_number": 39,
    "question": "A company is using two generative AI models: Model A has a context length of 2048 tokens and Model B has a context length of 8192 tokens. They need to apply an optimal chunking strategy for document processing while minimizing token wastage and ensuring response coherence. Which of the following strategies best suits this scenario?",
    "choices": [
      "Split documents into adaptive chunks that maximize the model's token limit while preserving semantic integrity.",
      "Preprocess all documents into 8192-token chunks and truncate them for Model A.",
      "Use a single chunk size of 1024 tokens for both models to ensure compatibility.",
      "Split the document into 256-token chunks to maintain uniformity across models."
    ],
    "correct_answer": "A",
    "is_multiple_choice": false
  },
  {
    "question_number": 40,
    "question": "A Generative AI Engineer is developing a document processing pipeline that will extract text from images and store it in a vector database for retrieval in a RAG-based system. The images contain a mix of English and Japanese text, and the engineer wants to ensure accurate extraction for both languages. Which approach should be taken to extract text from these multilingual images?",
    "choices": [
      "Use pytesseract with explicit language specification",
      "Use PyPDF2 for image-based text extraction",
      "Use Tika to parse text from images",
      "Use OpenCV for direct text extraction"
    ],
    "correct_answer": "A",
    "is_multiple_choice": false
  },
  {
    "question_number": 41,
    "question": "You have successfully trained a generative AI model and now need to deploy it in a production environment where latency and scalability are critical. Which of the following tools is best suited for serving the model?",
    "choices": [
      "MLflow Model Serving",
      "Jupyter Notebook",
      "Databricks Delta Sharing",
      "Apache Spark Streaming"
    ],
    "correct_answer": "A",
    "is_multiple_choice": false
  },
  {
    "question_number": 42,
    "question": "A Generative AI Engineer is building a pipeline that enhances a RAG system by extracting named entities (e.g., persons, organizations, locations) from text retrieved from documents. The engineer wants to use a Python library that provides pre-trained Named Entity Recognition (NER) models and can be easily integrated into an NLP pipeline. Which Python library should be used for this task?",
    "choices": [
      "spaCy",
      "gensim",
      "NLTK",
      "pytesseract"
    ],
    "correct_answer": "A",
    "is_multiple_choice": false
  },
  {
    "question_number": 43,
    "question": "A team building a RAG application faces degraded response quality due to irrelevant retrievals. Which TWO approaches improve retrieval by filtering extraneous content?",
    "choices": [
      "Apply heuristic-based text cleaning techniques like removing boilerplate content",
      "Disable stopword removal to retain all document words for retrieval",
      "Randomly remove 20% of content to reduce document length",
      "Use document chunking with semantic filtering to remove irrelevant sections",
      "Increase the chunk size to ensure more context is available in each retrieval"
    ],
    "correct_answer": "A, D",
    "is_multiple_choice": true
  },
  {
    "question_number": 44,
    "question": "You need to select a pre-trained model from a model hub for generating realistic images based on textual descriptions. The model metadata provides various details about each model. Which of the following metadata attributes is the most reliable indicator of a model's ability to generate high-quality, realistic images from text prompts?",
    "choices": [
      "Model type: Diffusion-based image generation model",
      "Model size: Contains fewer than 10 million parameters",
      "Fine-tuning history: Model fine-tuned for code completion",
      "Inference API available: Supports real-time deployment"
    ],
    "correct_answer": "A",
    "is_multiple_choice": false
  },
  {
    "question_number": 45,
    "question": "A company is using Databricks to build a generative AI-powered customer support assistant. They need to ensure that their prompts prevent private data leakage. Which TWO of the following strategies should they implement in their metaprompts?",
    "choices": [
      "Ensure that prompts instruct the model to focus on generic, anonymized patterns rather than specific user data.",
      "Store all user input as logs for future fine-tuning, regardless of whether they contain sensitive data.",
      "Allow the model to use historical customer interactions as training data to improve accuracy.",
      "Request that the model extract and summarize customer PII before responding to user queries.",
      "Explicitly instruct the model not to generate responses that include personal, confidential, or sensitive data."
    ],
    "correct_answer": "A, E",
    "is_multiple_choice": true
  }
]